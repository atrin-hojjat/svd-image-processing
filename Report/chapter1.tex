\chapter{بررسی روش \lr{SVD}}

\section{مقدمه}
روش 
\lr{SVD}
یکی از اساسی‌ترین روش‌های تجزیه ماتریس‌هاست و در حوزه‌های مختلفی مانند 
پردازیش سیگنال، آمار، یادگیری ماشین
\footnote{
  \lr{Machine Learning}
}
و \dots
کاربردهای بسیاری دارد.
در این پروژه، چند کاربرد آن در حوزه‌ی پردازش تصویر را بررسی و با روش‌های دیگر مقایسه خواهیم کرد.
به این منظور تصاویر را بصورت ماتریس‌های دوبعدی از تصاویر 
\lr{gray scale}
نشان خواهیم داد.

این روش، ماتریس را به سه ماتریس تجزیه می‌کند که این ماتریس‌ها از مقادیر ویژه‌ی 
\footnote{
  \lr{Eigen Values}
}
ماتریس اولیه به دست می‌آیند، به همین دلیل بسیاری از خواصی که این مقادیر ویژه دارند را خواهند داشت.

بطور کلی در این روش‌ها، از تجزیه
\lr{SVD}
استفاده می‌شود تا یک تقریب از ماتریس با رنک
\footnote{
  \lr{Rank}
}
پایینتر به دست بیاید.
این ماتریس می‌تواند تقریب خوبی برای 
تصویر اولیه با حجم دیتا کمتر باشد.
بعلاوه، می‌توان از مقادیر ویژه‌ی
پراهمیت تر این
ماتریس برای یافتن شباهت بین تصاویر استفاده کرد.

\section{روش \lr{SVD}}
برای یک ماتریس 
$n \times m$
دلخواه
$A$
،
تجزیه 
\lr{SVD}
این ماتریس به فرم

\begin{equation}
A = U \Sigma V^T
\end{equation}

نوشته می‌شود که در آن:

\begin{itemize}
  \item ماتریس‌های $U_{m \times m}$ و $V_{n \times n}$ متعامد اند
  \item $\Sigma_{m \times n}$ یک ماتریس قطری با عناصر نامنفی و نزولی است که به آنها \lr{Singular Values} گفته می‌شود
  \item به ستون‌های $V^T$ بردار‌های یکه راست و به ستون‌های $U$ بردار‌های یکه چپ گفته‌میشد
  \item $rank(\Sigma) = rank(A)$
\end{itemize}

در این‌تجزیه، به دلیل متعامد بودن
$U$
و
$V$
داریم 
$VV^T=V^TV=I_n$
و
$UU^T=U^TU=I_m$.
از طرفی، می‌توان با جذف کوچکترین مقادیر 
$\Sigma$
به تقریبی از 
$A$
با رنک کمتر دست یافت.



